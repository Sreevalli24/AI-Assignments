{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Question 1: Sentiment Analysis with Transformers\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import (\n",
        "    pipeline,\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "import time\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "xbWoQ0H5qPuM"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load IMDB dataset from TensorFlow Datasets\n",
        "print(\"Loading IMDB dataset...\")\n",
        "(train_ds, test_ds), info = tfds.load(\n",
        "    'imdb_reviews',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True\n",
        ")\n",
        "\n",
        "print(f\"Dataset info: {info.features}\")\n",
        "print(f\"Training samples: {info.splits['train'].num_examples}\")\n",
        "print(f\"Test samples: {info.splits['test'].num_examples}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPhB1fyrqfMm",
        "outputId": "b8512cc7-ce7e-400b-bd1f-89421b6f1b56"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Dataset info: FeaturesDict({\n",
            "    'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
            "    'text': Text(shape=(), dtype=string),\n",
            "})\n",
            "Training samples: 25000\n",
            "Test samples: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to lists for easier processing\n",
        "print(\"Processing dataset...\")\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for i, (text, label) in enumerate(test_ds.take(1000)):\n",
        "    test_texts.append(text.numpy().decode('utf-8'))\n",
        "    test_labels.append(label.numpy())\n",
        "    if i % 200 == 0:\n",
        "        print(f\"Processed {i+1} samples...\")\n",
        "\n",
        "print(f\"Processed {len(test_texts)} test samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91NI_olcqpOC",
        "outputId": "ee5e4b96-1bed-4252-d5e6-8dad1f5d7e2e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset...\n",
            "Processed 1 samples...\n",
            "Processed 201 samples...\n",
            "Processed 401 samples...\n",
            "Processed 601 samples...\n",
            "Processed 801 samples...\n",
            "Processed 1000 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of data\n",
        "print(f\"\\nSample review: {test_texts[0][:300]}...\")\n",
        "print(f\"Label: {'Positive' if test_labels[0] == 1 else 'Negative'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNrqnuHhqygy",
        "outputId": "2e3b16aa-1fb3-4b0d-8f26-1f9bfde985c9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample review: There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned ...\n",
            "Label: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading BERT sentiment analysis pipeline...\")\n",
        "try:\n",
        "    bert_classifier = pipeline(\n",
        "        \"sentiment-analysis\",\n",
        "        model=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
        "        return_all_scores=False\n",
        "    )\n",
        "except Exception as e:\n",
        "    bert_classifier = pipeline(\n",
        "        \"sentiment-analysis\",\n",
        "        model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "        return_all_scores=False\n",
        "    )\n",
        "\n",
        "print(\"Evaluating BERT model...\")\n",
        "bert_predictions = []\n",
        "bert_confidences = []\n",
        "\n",
        "start_time = time.time()\n",
        "for i, text in enumerate(test_texts[:500]):  # Use subset for faster processing\n",
        "    try:\n",
        "        truncated_text = text[:512]\n",
        "        result = bert_classifier(truncated_text)\n",
        "\n",
        "        # Convert to binary labels (0=negative, 1=positive)\n",
        "        if result[0]['label'] in ['POSITIVE', 'LABEL_2', '5 stars', '4 stars']:\n",
        "            bert_predictions.append(1)\n",
        "        else:\n",
        "            bert_predictions.append(0)\n",
        "\n",
        "        bert_confidences.append(result[0]['score'])\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Processed {i+1}/500 samples...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing sample {i}: {e}\")\n",
        "        bert_predictions.append(0)  # Default to negative\n",
        "        bert_confidences.append(0.5)\n",
        "\n",
        "bert_time = time.time() - start_time\n",
        "bert_accuracy = accuracy_score(test_labels[:len(bert_predictions)], bert_predictions)\n",
        "\n",
        "print(f\"BERT Processing Time: {bert_time:.2f} seconds\")\n",
        "print(f\"BERT Accuracy: {bert_accuracy:.4f}\")\n",
        "print(f\"Average Confidence: {np.mean(bert_confidences):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wr0IyIg9q_ZL",
        "outputId": "623fdfe9-4ed9-4f83-f620-b31e0eda4b9d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT sentiment analysis pipeline...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating BERT model...\n",
            "Processed 1/500 samples...\n",
            "Processed 101/500 samples...\n",
            "Processed 201/500 samples...\n",
            "Processed 301/500 samples...\n",
            "Processed 401/500 samples...\n",
            "BERT Processing Time: 285.95 seconds\n",
            "BERT Accuracy: 0.7900\n",
            "Average Confidence: 0.5418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading DistilBERT sentiment analysis pipeline...\")\n",
        "try:\n",
        "    distilbert_classifier = pipeline(\n",
        "        \"sentiment-analysis\",\n",
        "        model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "        return_all_scores=False\n",
        "    )\n",
        "except Exception as e:\n",
        "    distilbert_classifier = None\n",
        "\n",
        "if distilbert_classifier:\n",
        "    print(\"Evaluating DistilBERT model...\")\n",
        "    distilbert_predictions = []\n",
        "    distilbert_confidences = []\n",
        "\n",
        "    start_time = time.time()\n",
        "    for i, text in enumerate(test_texts[:500]):\n",
        "        try:\n",
        "            truncated_text = text[:512]\n",
        "            result = distilbert_classifier(truncated_text)\n",
        "\n",
        "            if result[0]['label'] == 'POSITIVE':\n",
        "                distilbert_predictions.append(1)\n",
        "            else:\n",
        "                distilbert_predictions.append(0)\n",
        "\n",
        "            distilbert_confidences.append(result[0]['score'])\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Processed {i+1}/500 samples...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing sample {i}: {e}\")\n",
        "            distilbert_predictions.append(0)\n",
        "            distilbert_confidences.append(0.5)\n",
        "\n",
        "    distilbert_time = time.time() - start_time\n",
        "    distilbert_accuracy = accuracy_score(test_labels[:len(distilbert_predictions)], distilbert_predictions)\n",
        "\n",
        "    print(f\"DistilBERT Processing Time: {distilbert_time:.2f} seconds\")\n",
        "    print(f\"DistilBERT Accuracy: {distilbert_accuracy:.4f}\")\n",
        "    print(f\"Average Confidence: {np.mean(distilbert_confidences):.4f}\")\n",
        "else:\n",
        "    distilbert_accuracy = 0\n",
        "    distilbert_time = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8F1vRHzrIuJ",
        "outputId": "70b994fd-d839-4045-9616-84fe81767f75"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading DistilBERT sentiment analysis pipeline...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating DistilBERT model...\n",
            "Processed 1/500 samples...\n",
            "Processed 101/500 samples...\n",
            "Processed 201/500 samples...\n",
            "Processed 301/500 samples...\n",
            "Processed 401/500 samples...\n",
            "DistilBERT Processing Time: 126.10 seconds\n",
            "DistilBERT Accuracy: 0.8340\n",
            "Average Confidence: 0.9774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading RoBERTa sentiment analysis pipeline...\")\n",
        "try:\n",
        "    roberta_classifier = pipeline(\n",
        "        \"sentiment-analysis\",\n",
        "        model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "        return_all_scores=True\n",
        "    )\n",
        "except Exception as e:\n",
        "    roberta_classifier = None\n",
        "\n",
        "if roberta_classifier:\n",
        "    print(\"Evaluating RoBERTa model...\")\n",
        "    roberta_predictions = []\n",
        "    roberta_confidences = []\n",
        "\n",
        "    start_time = time.time()\n",
        "    for i, text in enumerate(test_texts[:500]):\n",
        "        try:\n",
        "            truncated_text = text[:512]\n",
        "            results = roberta_classifier(truncated_text)\n",
        "\n",
        "            # Find positive sentiment (LABEL_2)\n",
        "            pos_score = next((r['score'] for r in results[0] if r['label'] == 'LABEL_2'), 0)\n",
        "            neg_score = next((r['score'] for r in results[0] if r['label'] == 'LABEL_0'), 0)\n",
        "\n",
        "            if pos_score > neg_score:\n",
        "                roberta_predictions.append(1)\n",
        "                roberta_confidences.append(pos_score)\n",
        "            else:\n",
        "                roberta_predictions.append(0)\n",
        "                roberta_confidences.append(neg_score)\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Processed {i+1}/500 samples...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing sample {i}: {e}\")\n",
        "            roberta_predictions.append(0)\n",
        "            roberta_confidences.append(0.5)\n",
        "\n",
        "    roberta_time = time.time() - start_time\n",
        "    roberta_accuracy = accuracy_score(test_labels[:len(roberta_predictions)], roberta_predictions)\n",
        "\n",
        "    print(f\"RoBERTa Processing Time: {roberta_time:.2f} seconds\")\n",
        "    print(f\"RoBERTa Accuracy: {roberta_accuracy:.4f}\")\n",
        "    print(f\"Average Confidence: {np.mean(roberta_confidences):.4f}\")\n",
        "else:\n",
        "    roberta_accuracy = 0\n",
        "    roberta_time = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fyilriTrTx_",
        "outputId": "a372062c-1168-4c6b-b14d-444a0471776d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading RoBERTa sentiment analysis pipeline...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating RoBERTa model...\n",
            "Processed 1/500 samples...\n",
            "Processed 101/500 samples...\n",
            "Processed 201/500 samples...\n",
            "Processed 301/500 samples...\n",
            "Processed 401/500 samples...\n",
            "RoBERTa Processing Time: 238.54 seconds\n",
            "RoBERTa Accuracy: 0.5020\n",
            "Average Confidence: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#comparision\n",
        "models_data = []\n",
        "if bert_accuracy > 0:\n",
        "    models_data.append(['BERT-base', bert_accuracy, bert_time, '110M'])\n",
        "if distilbert_accuracy > 0:\n",
        "    models_data.append(['DistilBERT', distilbert_accuracy, distilbert_time, '66M'])\n",
        "if roberta_accuracy > 0:\n",
        "    models_data.append(['RoBERTa', roberta_accuracy, roberta_time, '125M'])\n",
        "\n",
        "if models_data:\n",
        "    results_df = pd.DataFrame(models_data, columns=['Model', 'Accuracy', 'Time(s)', 'Parameters'])\n",
        "    print(results_df.to_string(index=False))\n",
        "\n",
        "    # Find best model\n",
        "    best_model_idx = results_df['Accuracy'].idxmax()\n",
        "    best_model = results_df.iloc[best_model_idx]\n",
        "\n",
        "    print(f\"\\n BEST PERFORMING MODEL: {best_model['Model']}\")\n",
        "    print(f\"Accuracy: {best_model['Accuracy']:.4f}\")\n",
        "    print(f\"Processing Time: {best_model['Time(s)']:.2f} seconds\")\n",
        "\n",
        "    # Detailed analysis for best model\n",
        "    print(\"\\nDETAILED CLASSIFICATION REPORT (Best Model):\")\n",
        "    if best_model['Model'] == 'BERT-base':\n",
        "        print(classification_report(test_labels[:len(bert_predictions)], bert_predictions,\n",
        "                                  target_names=['Negative', 'Positive']))\n",
        "    elif best_model['Model'] == 'DistilBERT':\n",
        "        print(classification_report(test_labels[:len(distilbert_predictions)], distilbert_predictions,\n",
        "                                  target_names=['Negative', 'Positive']))\n",
        "    elif best_model['Model'] == 'RoBERTa':\n",
        "        print(classification_report(test_labels[:len(roberta_predictions)], roberta_predictions,\n",
        "                                  target_names=['Negative', 'Positive']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk4Wmq3LrfG5",
        "outputId": "7d3e418c-2dc6-4d45-f48e-abecb6fdb86a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Model  Accuracy    Time(s) Parameters\n",
            " BERT-base     0.790 285.951953       110M\n",
            "DistilBERT     0.834 126.103485        66M\n",
            "   RoBERTa     0.502 238.536366       125M\n",
            "\n",
            " BEST PERFORMING MODEL: DistilBERT\n",
            "Accuracy: 0.8340\n",
            "Processing Time: 126.10 seconds\n",
            "\n",
            "DETAILED CLASSIFICATION REPORT (Best Model):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.81      0.87      0.84       251\n",
            "    Positive       0.86      0.80      0.83       249\n",
            "\n",
            "    accuracy                           0.83       500\n",
            "   macro avg       0.84      0.83      0.83       500\n",
            "weighted avg       0.84      0.83      0.83       500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 2: TEXT GENERATION WITH TRANSFORMERS\n",
        "\n",
        "# Load GPT-2 model for text generation\n",
        "try:\n",
        "    text_generator = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=\"gpt2\",\n",
        "        tokenizer=\"gpt2\",\n",
        "        device=-1\n",
        "    )\n",
        "    print(\"GPT-2 model loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\" Error loading GPT-2: {e}\")\n",
        "    text_generator = None\n",
        "\n",
        "if text_generator:\n",
        "    # Given prompt\n",
        "    prompt = \"In a distant future, humanity has discovered\"\n",
        "\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(\"\\n\" + \"-\" * 60)\n",
        "    print(\"GENERATED STORIES:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Generate multiple versions with different parameters\n",
        "    generation_configs = [\n",
        "        {\"max_length\": 150, \"temperature\": 0.7, \"do_sample\": True, \"top_p\": 0.9, \"name\": \"Balanced\"},\n",
        "        {\"max_length\": 150, \"temperature\": 0.5, \"do_sample\": True, \"top_k\": 50, \"name\": \"Conservative\"},\n",
        "        {\"max_length\": 150, \"temperature\": 0.9, \"do_sample\": True, \"top_p\": 0.8, \"name\": \"Creative\"}\n",
        "    ]\n",
        "\n",
        "    for i, config in enumerate(generation_configs, 1):\n",
        "        print(f\"\\nSTORY {i} - {config['name']} (Temperature: {config['temperature']}):\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        try:\n",
        "            generated = text_generator(\n",
        "                prompt,\n",
        "                max_length=config[\"max_length\"],\n",
        "                temperature=config[\"temperature\"],\n",
        "                do_sample=config[\"do_sample\"],\n",
        "                top_p=config.get(\"top_p\", 1.0),\n",
        "                top_k=config.get(\"top_k\", 0),\n",
        "                pad_token_id=50256,\n",
        "                num_return_sequences=1,\n",
        "                truncation=True\n",
        "            )\n",
        "\n",
        "            story = generated[0]['generated_text']\n",
        "            print(story)\n",
        "            print(f\"\\nLength: {len(story.split())} words\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating story: {e}\")\n",
        "\n",
        "    # ADDITIONAL TEXT GENERATION EXAMPLES\n",
        "    additional_prompts = [\n",
        "        \"The last library on Earth contained\",\n",
        "        \"When artificial intelligence gained consciousness, it\",\n",
        "        \"In the depths of the ocean, scientists discovered\"\n",
        "    ]\n",
        "\n",
        "    for j, prompt in enumerate(additional_prompts, 1):\n",
        "        print(f\"\\n{j}. Prompt: '{prompt}'\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        try:\n",
        "            generated = text_generator(\n",
        "                prompt,\n",
        "                max_length=120,\n",
        "                temperature=0.8,\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                pad_token_id=50256,\n",
        "                num_return_sequences=1,\n",
        "                truncation=True\n",
        "            )\n",
        "\n",
        "            story = generated[0]['generated_text']\n",
        "            print(story)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating story: {e}\")"
      ],
      "metadata": {
        "id": "GHDqCI9qg2Zw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d880d44e-a7aa-47e1-afc1-fde81d0fc19c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-2 model loaded successfully\n",
            "Prompt: 'In a distant future, humanity has discovered'\n",
            "\n",
            "------------------------------------------------------------\n",
            "GENERATED STORIES:\n",
            "------------------------------------------------------------\n",
            "\n",
            "STORY 1 - Balanced (Temperature: 0.7):\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a distant future, humanity has discovered the secrets of the past and the secrets of the future.\n",
            "\n",
            "The future of humanity is not a living future, but an extinct one. We are at a crossroads. Our civilization is at a crossroads. We can no longer be the people who have lived through the past. We have to make our way through the present. We have to make our way through the future.\n",
            "\n",
            "I'll be speaking at the National Public Radio Conference on September 28, 2013, in Las Vegas, Nevada.\n",
            "\n",
            "The following is an excerpt from a talk I gave last year in New Orleans, Louisiana, on the topic of the future of civilization.\n",
            "\n",
            "\"I'm talking about the future of civilization,\" you say. \"We are at a crossroads.\"\n",
            "\n",
            "You are right.\n",
            "\n",
            "I know it's difficult to say this, but I'll give you an example. The past is not the future. The past is the future. We are living through an old era. We are living through an age of scarcity and a time of loss. We are living through an age of overpopulation. We are living through an age of disease. We are living through an age of climate change. We are living through an age of wars. We are living through\n",
            "\n",
            "Length: 212 words\n",
            "\n",
            "STORY 2 - Conservative (Temperature: 0.5):\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a distant future, humanity has discovered that it is possible to create an alternative energy source, a source of energy that is not only more efficient but is also more sustainable.\n",
            "\n",
            "\"We need to get to the point where we can build a system that is sustainable,\" said Dr. A.J. Gurney, a physicist at the University of California, Los Angeles.\n",
            "\n",
            "The idea is to build a system that is sustainable only by applying a certain amount of energy to the environment. The system would be able to produce a high level of energy from sunlight, which is needed to power a turbine. The system would also be able to use sunlight to generate electricity.\n",
            "\n",
            "\"We're doing it in a way that is sustainable, but it's not sustainable in a way that is sustainable in a way that is sustainable in a way that is sustainable in a way that is sustainable in a way that is sustainable in a way that is sustainable in a way that is sustainable in a way that is sustainable in a way that is sustainable in a way that is sustainable in a way that is sustainable in a way that is sustainable in a way that is sustainable in a way that is sustainable in a way that is sustainable in a way that is sustainable in a way that is sustainable in a way that is sustainable\n",
            "\n",
            "Length: 236 words\n",
            "\n",
            "STORY 3 - Creative (Temperature: 0.9):\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a distant future, humanity has discovered a way to solve some of the world's most pressing problems: climate change and the nuclear option. This kind of technology could make a huge difference in the lives of millions of people. But how do we know what happens to our world?\n",
            "\n",
            "In a world of constant conflict, the \"fire and fury\" of war has been both compelling and incredibly detrimental to the very survival of humankind. A few of us are simply better off today than we were when we were first born. But these differences have caused a great deal of stress for some of us.\n",
            "\n",
            "Take my dad. A man who spends most of his time in a wheelchair and constantly suffers from a debilitating condition that will likely be difficult to diagnose. While he has his own company, I have the honor of serving in a team that includes Dr. Neuberger, who also happens to be a nuclear physicist. In fact, he is the first member of the team to be awarded the Nobel Prize in Physiology or Medicine in 1993.\n",
            "\n",
            "I feel like he is an amazing human being. But his problems stem from a lack of energy. As a result, he's been living with his mother for more than a decade. In fact, my dad still has an energy deficiency, but he's struggling\n",
            "\n",
            "Length: 227 words\n",
            "\n",
            "1. Prompt: 'The last library on Earth contained'\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The last library on Earth contained a very small number of books, many of which were devoted to the Bible. But some of them were not very long, and some were only about a year old.\n",
            "\n",
            "It was the first time in our history that any book had been preserved. The oldest book on the planet is a little over a century old, and its contents are quite extensive. It is a large collection of manuscripts of the Bible which was published in 1620. The oldest manuscripts were collected in 1836, and were in use from 1872 until 1876. It is still a very interesting book.\n",
            "\n",
            "The book is made up of a number of sections, some of which are very long, and some of which are very short. The main section in particular is very short. It consists of a long introduction to the subject.\n",
            "\n",
            "The Bible contains many other documents, especially the Book of Mormon. It is the only major document on the planet that contains the Bible in its entirety.\n",
            "\n",
            "The book contains many other items, including a copy of the Bible and other books, a translation of the Book of Mormon, a translation of the Bible by an Egyptian, and a translation of the Bible by an American.\n",
            "\n",
            "We know that the Bible contains the most important and most important\n",
            "\n",
            "2. Prompt: 'When artificial intelligence gained consciousness, it'\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When artificial intelligence gained consciousness, it became increasingly difficult for the human race to comprehend the truth, or even to understand the world around us.\n",
            "\n",
            "This is why the AI revolution has always been difficult, because the human race has been in the forefront of the battle against AI and has been fighting against the very notion of being a \"superintelligence\".\n",
            "\n",
            "To give you an idea of the problem of human versus AI, imagine that you are a child who is playing with your grandfather's dog in a park. Your grandfather has been fighting the evil AI for over a year, and it's getting more and more vicious, and your grandmother is watching over you.\n",
            "\n",
            "The two of you will have different stories, and in the end your grandfather's dog is going to kill you.\n",
            "\n",
            "This is what happens when you don't have the tools to deal with the evil AI.\n",
            "\n",
            "This is what happens when you don't have the tools to deal with the evil AI.\n",
            "\n",
            "You're not smart enough to fight it. You're not smart enough to understand what's happening, and you don't know what to do.\n",
            "\n",
            "And then when you do understand what's happening, you're not smart enough to fight the evil AI.\n",
            "\n",
            "This is what happens when you don't have\n",
            "\n",
            "3. Prompt: 'In the depths of the ocean, scientists discovered'\n",
            "------------------------------\n",
            "In the depths of the ocean, scientists discovered that all three of the four carbon isotopes that give off their chemical names are the same: carbon dioxide, carbon dioxide 2.5 and carbon dioxide 1.5.\n",
            "\n",
            "Now, scientists are using the new results to make a new determination about the molecular makeup of these isotopes.\n",
            "\n",
            "They found that the isotopes in the ocean mix differently.\n",
            "\n",
            "\"We've seen that carbon dioxide has two, three, four and five carbon atoms in it, but it's not as if it's all that different,\" said study co-author and marine scientist Christopher Schmitt, a marine biologist at the University of Bristol in the UK. \"It's more like that of a lot of other organic matter that's trapped in the ocean.\"\n",
            "\n",
            "The researchers say they have uncovered an unusual phenomenon in the ocean, where there are more carbon atoms in the ocean than in the atmosphere.\n",
            "\n",
            "\"We have observed that carbon dioxide is only one carbon atom,\" Schmitt said. \"So there are only about 100 carbon atoms in the ocean. So the oceans are getting a lot more carbon in the ocean.\"\n",
            "\n",
            "The new study is published in Nature Geoscience.\n",
            "\n",
            "More than 3,000 scientists from the U.S. Geological Survey (USGS\n"
          ]
        }
      ]
    }
  ]
}